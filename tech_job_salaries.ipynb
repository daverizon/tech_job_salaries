{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Which tech job pays the most base salary on average?\n",
    "\n",
    "2) Which tech jobs pays the most total yearly compensation on average?\n",
    "\n",
    "3) What is the average number of years people stay at a company?\n",
    "\n",
    "4) Do males have a larger overall annual compensation than females in the tech industry?\n",
    "\n",
    "5) What are the biggest contributors to the overall annual compensation you make?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "# import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "# Data was obtained from Kaggle \"Data Science and STEM Salaries\" \n",
    "# (https://www.kaggle.com/datasets/jackogozaly/data-science-and-stem-salaries/versions/1?resource=download)\n",
    "\n",
    "df = pd.read_csv('./Levels_Fyi_Salary_Data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like it may be already one-hot encoded for race\n",
    "racedf = df[['Race_Asian', 'Race_White', 'Race_Two_Or_More', 'Race_Black', 'Race_Hispanic', 'Race']]\n",
    "racedf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "racedf[racedf['Race'].notnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like it may be already one-hot encoded for education\n",
    "educationdf = df[['Masters_Degree', 'Bachelors_Degree', 'Doctorate_Degree', 'Highschool', 'Some_College', 'Education']]\n",
    "educationdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "educationdf[educationdf['Education'].notnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "\n",
    "- Race and Education are both one-hot encoded already so can drop those columns and only use the one-hot encoding when predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Respondent distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the distribution of company respondents that are in this data?\n",
    "valueCounts = df['company'].value_counts()\n",
    "valueCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "companies = list(valueCounts.index)[:20]\n",
    "respondents = list(valueCounts.values)[:20]\n",
    "ax.bar(companies,respondents)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Company', fontsize=12)\n",
    "plt.ylabel('Respondents', fontsize=12)\n",
    "plt.title('Respondent Companies', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the distribution of job titles that are in this data?\n",
    "df['title'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "\n",
    "- Top 10 companies respondents belong to are: Amazon, Microsoft, Google, Facebook, Apple, Oracle, Salesforce, Intel, Cisco, IBM\n",
    "\n",
    "- Overwhelming number of respondents are Software Engineering.  Would be good to downsample for predictions if had more time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean and max base salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the mean base salary for each of the job titles\n",
    "meanBaseSalaries = df[['title', 'basesalary']].groupby(['title']).mean().rename(columns={\"basesalary\": \"meanBaseSalary\"})\n",
    "meanBaseSalaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the max base salary for each of the job titles\n",
    "maxBaseSalaries = df[['title', 'basesalary']].groupby(['title']).max().rename(columns={\"basesalary\": \"maxBaseSalary\"})\n",
    "maxBaseSalaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salarydf = meanBaseSalaries.join(maxBaseSalaries, on=['title']).sort_values(by=['meanBaseSalary'], ascending=False)\n",
    "salarydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = salarydf.shape[0]\n",
    "\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.35       # the width of the bars\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = fig.add_subplot(111)\n",
    "rects1 = ax.bar(ind, list(salarydf['meanBaseSalary']), width, color='#069AF3')\n",
    "rects2 = ax.bar(ind+width, list(salarydf['maxBaseSalary']), width, color='#0047AB')\n",
    "\n",
    "# add some\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels( list(salarydf.index) )\n",
    "\n",
    "ax.legend( (rects1[0], rects2[0]), ('Mean Base Salary', 'Max Base Salary') )\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Job Titles', fontsize=12)\n",
    "plt.ylabel('Salary', fontsize=12)\n",
    "plt.title('Base Salaries Based on Job Title', fontsize=15)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Look at mean/max base salary based on company instead of job title\n",
    "\n",
    "- Look at only top 20 companies for mean base salary since there are so many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanBaseSalaries_company = df[['company', 'basesalary']].groupby(['company']).mean().rename(columns={\"basesalary\": \"meanBaseSalary\"})\n",
    "maxBaseSalaries_company = df[['company', 'basesalary']].groupby(['company']).max().rename(columns={\"basesalary\": \"maxBaseSalary\"})\n",
    "salarydf_company = meanBaseSalaries_company.join(maxBaseSalaries_company, on=['company']).sort_values(by=['meanBaseSalary'], ascending=False).head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = salarydf_company.shape[0]\n",
    "\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.35       # the width of the bars\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = fig.add_subplot(111)\n",
    "rects1 = ax.bar(ind, list(salarydf_company['meanBaseSalary']), width, color='#069AF3')\n",
    "rects2 = ax.bar(ind+width, list(salarydf_company['maxBaseSalary']), width, color='#0047AB')\n",
    "\n",
    "# add some\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels( list(salarydf_company.index) )\n",
    "\n",
    "ax.legend( (rects1[0], rects2[0]), ('Mean Base Salary', 'Max Base Salary') )\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Company', fontsize=12)\n",
    "plt.ylabel('Salary', fontsize=12)\n",
    "plt.title('Base Salaries Based on Company', fontsize=15)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same analyis as above but for location instead of title/company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanBaseSalaries_location = df[['location', 'basesalary']].groupby(['location']).mean().rename(columns={\"basesalary\": \"meanBaseSalary\"})\n",
    "maxBaseSalaries_location = df[['location', 'basesalary']].groupby(['location']).max().rename(columns={\"basesalary\": \"maxBaseSalary\"})\n",
    "salarydf_location = meanBaseSalaries_location.join(maxBaseSalaries_location, on=['location']).sort_values(by=['meanBaseSalary'], ascending=False).head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = salarydf_location.shape[0]\n",
    "\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.35       # the width of the bars\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = fig.add_subplot(111)\n",
    "rects1 = ax.bar(ind, list(salarydf_location['meanBaseSalary']), width, color='#069AF3')\n",
    "rects2 = ax.bar(ind+width, list(salarydf_location['maxBaseSalary']), width, color='#0047AB')\n",
    "\n",
    "# add some\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels( list(salarydf_location.index) )\n",
    "\n",
    "ax.legend( (rects1[0], rects2[0]), ('Mean Base Salary', 'Max Base Salary') )\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Location', fontsize=12)\n",
    "plt.ylabel('Salary', fontsize=12)\n",
    "plt.title('Base Salaries Based on Location', fontsize=15)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "\n",
    "- The upper-bound for max base salary is very high compared to the mean base salary.\n",
    "\n",
    "- Mean and max base salary stay pretty consistent for most companies except for a few like Netflix, Roku, Doordash, Brex.  Note that Netflix and Squarespace have the largest max base salary available which is where the high Product Management and Software Engineer jobs are\n",
    "\n",
    "- Base salaries seem higher in CA and NJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean and max total annual compensation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the mean base salary for each of the job titles\n",
    "meanAnnualComp = df[['title', 'totalyearlycompensation']].groupby(['title']).mean().rename(columns={\"totalyearlycompensation\": \"meanAnnualComp\"})\n",
    "meanAnnualComp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the max base salary for each of the job titles\n",
    "maxAnnualComp = df[['title', 'totalyearlycompensation']].groupby(['title']).max().rename(columns={\"totalyearlycompensation\": \"maxAnnualComp\"})\n",
    "maxAnnualComp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annualCompdf = meanAnnualComp.join(maxAnnualComp, on=['title']).sort_values(by=['meanAnnualComp'], ascending=False)\n",
    "annualCompdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = annualCompdf.shape[0]\n",
    "\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.35       # the width of the bars\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = fig.add_subplot(111)\n",
    "rects1 = ax.bar(ind, list(annualCompdf['meanAnnualComp']), width, color='#069AF3')\n",
    "rects2 = ax.bar(ind+width, list(annualCompdf['maxAnnualComp']), width, color='#0047AB')\n",
    "\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels( list(annualCompdf.index) )\n",
    "\n",
    "ax.legend( (rects1[0], rects2[0]), ('Mean Annual Compensation', 'Max Annual Compensation') )\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Job Titles', fontsize=12)\n",
    "plt.ylabel('Annual Compensation', fontsize=12)\n",
    "plt.title('Annual Compensation Based on Job Title', fontsize=15)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Look at mean/max annual compensation based on company instead of job title\n",
    "\n",
    "- Look at only top 20 company mean annual compensation since there are so many companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanAnnualComp_company = df[['company', 'totalyearlycompensation']].groupby(['company']).mean().rename(columns={\"totalyearlycompensation\": \"meanAnnualComp\"})\n",
    "maxAnnualComp_company = df[['company', 'totalyearlycompensation']].groupby(['company']).max().rename(columns={\"totalyearlycompensation\": \"maxAnnualComp\"})\n",
    "annualCompdf_company = meanAnnualComp_company.join(maxAnnualComp_company, on=['company']).sort_values(by=['meanAnnualComp'], ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = annualCompdf_company.shape[0]\n",
    "\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.35       # the width of the bars\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = fig.add_subplot(111)\n",
    "rects1 = ax.bar(ind, list(annualCompdf_company['meanAnnualComp']), width, color='#069AF3')\n",
    "rects2 = ax.bar(ind+width, list(annualCompdf_company['maxAnnualComp']), width, color='#0047AB')\n",
    "\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels( list(annualCompdf_company.index) )\n",
    "\n",
    "ax.legend( (rects1[0], rects2[0]), ('Mean Annual Compensation', 'Max Annual Compensation') )\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Company', fontsize=12)\n",
    "plt.ylabel('Annual Compensation', fontsize=12)\n",
    "plt.title('Annual Compensation Based on Company', fontsize=15)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same analysis but based on location instead of title/company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanAnnualComp_location = df[['location', 'totalyearlycompensation']].groupby(['location']).mean().rename(columns={\"totalyearlycompensation\": \"meanAnnualComp\"})\n",
    "maxAnnualComp_location = df[['location', 'totalyearlycompensation']].groupby(['location']).max().rename(columns={\"totalyearlycompensation\": \"maxAnnualComp\"})\n",
    "annualCompdf_location = meanAnnualComp_location.join(maxAnnualComp_location, on=['location']).sort_values(by=['meanAnnualComp'], ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annualCompdf_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = annualCompdf_location.shape[0]\n",
    "\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.35       # the width of the bars\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = fig.add_subplot(111)\n",
    "rects1 = ax.bar(ind, list(annualCompdf_location['meanAnnualComp']), width, color='#069AF3')\n",
    "rects2 = ax.bar(ind+width, list(annualCompdf_location['maxAnnualComp']), width, color='#0047AB')\n",
    "\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels( list(annualCompdf_location.index) )\n",
    "\n",
    "ax.legend( (rects1[0], rects2[0]), ('Mean Annual Compensation', 'Max Annual Compensation') )\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Location', fontsize=12)\n",
    "plt.ylabel('Annual Compensation', fontsize=12)\n",
    "plt.title('Annual Compensation Based on Location', fontsize=15)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['totalyearlycompensation'] == 4980000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "\n",
    "- The upper-bound for annual total compensation is very high compared to the mean annual total compensation.\n",
    "\n",
    "- Within companies, there are some which have very large max annual compensation compared to the mean annual compensation meaning some companies are able to offer their employees considerable perks \n",
    "\n",
    "- There is one very clear outlier where their total annual compensation is much much larger than the average for that area.  They work at Facebook in Menlo Park, CA.  Besides this most areas don't have a instances where their employees are able to get more annual compensation that the average.  Exceptions are San Mateo, FL; Lost Gastos, CA; Los Altos, CA \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average number of years at a company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['yearsatcompany'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "\n",
    "- Most people stay with the company for only a few years before moving on since the number of people with more years at the same company drastically decreases after 2 years\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender comparison of salaries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'll only look at male-vs-female overall compensation since that's what I'll be modeling later in this notebook.  I'll also group by job title since from previous plots it was evident job title is a big driving force for what salary you get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there are many NaN's for the gender column, I'll filter all those out since I'm specifically interested in male-vs-female gender bias\n",
    "# For the same reason I'll filter out the 'Other' gender so I only have 'Male' and 'Female'\n",
    "genderbiasdf = df[['gender', 'title', 'totalyearlycompensation']].dropna()\n",
    "genderbiasdf = genderbiasdf[(genderbiasdf['gender'] == 'Male') | (genderbiasdf['gender'] == 'Female')]\n",
    "genderbiasdf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genderPlotdf = genderbiasdf.groupby(['gender', 'title']).mean().reset_index()\n",
    "genderPlotdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = len(list(genderPlotdf['title'].unique()))\n",
    "\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.35       # the width of the bars\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = fig.add_subplot(111)\n",
    "rects1 = ax.bar(ind, list(genderPlotdf[genderPlotdf['gender'] == 'Male']['totalyearlycompensation']), width, color='#069AF3')\n",
    "rects2 = ax.bar(ind+width, list(genderPlotdf[genderPlotdf['gender'] == 'Female']['totalyearlycompensation']), width, color='#D2042D')\n",
    "\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels( list(genderPlotdf['title'].unique()) )\n",
    "\n",
    "ax.legend( (rects1[0], rects2[0]), ('Male Annual Compensation', 'Female Annual Compensation') )\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Job Titles', fontsize=12)\n",
    "plt.ylabel('Annual Compensation', fontsize=12)\n",
    "plt.title('Annual Compensation For Male Vs Female', fontsize=15)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "\n",
    "- We do see a clear gender bias in the data for total annual compensation\n",
    "\n",
    "- Titles with a discernable gender bias are: Human Resources, Marketing, Product Designer, and Product Manager\n",
    "\n",
    "- If I had more time, would be nice to see if there's any dependency based on Company and/or location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Dominant features for predicting salary (total annual compensation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will be using the same steps as done in the Udacity course for predicting salary with some minor tweaks based on the one-hot encoding since some are already done for us in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    '''\n",
    "    INPUT\n",
    "    df - pandas dataframe \n",
    "    \n",
    "    OUTPUT\n",
    "    X - A matrix holding all of the variables you want to consider when predicting the response\n",
    "    y - the corresponding response vector\n",
    "    \n",
    "    '''\n",
    "#     Drop all the rows with no total annual compensation    \n",
    "    df_new = df.dropna(subset=['totalyearlycompensation'])\n",
    "    \n",
    "    # Drop the following columns from X:\n",
    "    # timestamp: Not meaningful since it's only the timestamp that the answer was recorded\n",
    "    # Race: It's already one-hot encoded so all the information is included in the other columns\n",
    "    # Education: It's already one-hot encoded so all the information is included in the other columns\n",
    "    # basesalary: Related to totalyealycompensation so I'll remove this feature\n",
    "    # stockgrantvalue: Related to totalyealycompensation so I'll remove this feature\n",
    "    # bonus: Related to totalyealycompensation so I'll remove this feature\n",
    "    # otherdetails: Since this is a free-form field that's filled out by the user\n",
    "    # rowNumber: Since this is just an identifier value\n",
    "    # cityid: Since this information is contained in the 'location' column which has no nulls\n",
    "    # dmaid: Since this information is contained in the 'location' column which has no nulls\n",
    "    df_new = df_new.drop(['timestamp', 'Race', 'Education', 'basesalary', 'stockgrantvalue', 'bonus', 'otherdetails', 'rowNumber', 'cityid', 'dmaid'], axis=1)\n",
    "    \n",
    "    # One gender row seems to be an error, the value is 'Title: Senior Software Engineer', I'll drop that row\n",
    "    # 1/3 of the gender columns is missing values, so I'll fill those with 'NA' and later I'll do one-hot encoding for gender\n",
    "    # Only 5 rows of data don't have a company specified.  Since it's so little I'll drop those entries\n",
    "    # For the rows where column tag or level are missing, I'll fill it with 'NA'\n",
    "    df_new = df_new[df_new['gender'] != 'Title: Senior Software Engineer']\n",
    "    df_new['gender'] = df_new['gender'].fillna('NA')\n",
    "    df_new = df_new[df_new['company'].notnull()]\n",
    "    df_new['tag'] = df_new['tag'].fillna('NA')\n",
    "    df_new['level'] = df_new['level'].fillna('NA')\n",
    "    \n",
    "    # Later when doing the fitting, I noticed that it wouldn't error out, but it wouldn't complete.   I think because there were too many categorical columns that were one-hot encoded.\n",
    "    # I'll reduce the df to only companies that have more than 100 respondents for that company and location\n",
    "    df_new['company__location'] = df_new['company'] + '__' + df['location']\n",
    "    respondents = df_new['company__location'].value_counts() > 100\n",
    "    largeRespondents = list(respondents[respondents.values].index)\n",
    "    df_new = df_new[df_new['company__location'].isin(largeRespondents)]\n",
    "    # Drop the new column that was created since it's not needed anymore\n",
    "    df_new = df_new.drop(['company__location'], axis=1)\n",
    "\n",
    "    # The only numerical columns I'd impute are yearsofexperience, yearsatcompany however looks like there are no missing values for these columns\n",
    "\n",
    "#   Create X as all the columns that are not the total annual compensation column\n",
    "    X = df_new.drop(['totalyearlycompensation'], axis=1)\n",
    "\n",
    "#   Create y as the totalyearlycompensation column\n",
    "    y = df_new['totalyearlycompensation']\n",
    "        \n",
    "#   Create dummy columns for all the categorical variables in X that are left\n",
    "    cat_X_helper = X.select_dtypes(include=['object'])  \n",
    "    cat_X = pd.get_dummies(cat_X_helper, prefix=list(cat_X_helper.columns))\n",
    "\n",
    "    #Concat numerical and categorical columns back together\n",
    "    X = pd.concat([X.select_dtypes(exclude=['object'])  , cat_X], axis=1)\n",
    "    \n",
    "    return X, y\n",
    "    \n",
    "#Use the function to create X and y\n",
    "X, y = clean_data(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run various cutoffs for features like how it was done during the exercises to find the optimal features that should be included in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_lm_mod(X, y, cutoffs, test_size = .30, random_state=42, plot=True):\n",
    "    '''\n",
    "    INPUT\n",
    "    X - pandas dataframe, X matrix\n",
    "    y - pandas dataframe, response variable\n",
    "    cutoffs - list of ints, cutoff for number of non-zero values in dummy categorical vars\n",
    "    test_size - float between 0 and 1, default 0.3, determines the proportion of data as test data\n",
    "    random_state - int, default 42, controls random state for train_test_split\n",
    "    plot - boolean, default 0.3, True to plot result\n",
    "\n",
    "    OUTPUT\n",
    "    r2_scores_test - list of floats of r2 scores on the test data\n",
    "    r2_scores_train - list of floats of r2 scores on the train data\n",
    "    lm_model - model object from sklearn\n",
    "    X_train, X_test, y_train, y_test - output from sklearn train test split used for optimal model\n",
    "    '''\n",
    "    r2_scores_test, r2_scores_train, num_feats, results = [], [], [], dict()\n",
    "    for cutoff in cutoffs:\n",
    "\n",
    "        #reduce X matrix\n",
    "        reduce_X = X.iloc[:, np.where((X.sum() > cutoff) == True)[0]]\n",
    "        num_feats.append(reduce_X.shape[1])\n",
    "\n",
    "        #split the data into train and test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(reduce_X, y, test_size = test_size, random_state=random_state)\n",
    "\n",
    "        #fit the model and obtain pred response\n",
    "        lm_model = LinearRegression(normalize=True)\n",
    "        lm_model.fit(X_train, y_train)\n",
    "        y_test_preds = lm_model.predict(X_test)\n",
    "        y_train_preds = lm_model.predict(X_train)\n",
    "\n",
    "        #append the r2 value from the test set\n",
    "        r2_scores_test.append(r2_score(y_test, y_test_preds))\n",
    "        r2_scores_train.append(r2_score(y_train, y_train_preds))\n",
    "        results[str(cutoff)] = r2_score(y_test, y_test_preds)\n",
    "\n",
    "    if plot:\n",
    "        plt.plot(num_feats, r2_scores_test, label=\"Test\", alpha=.5)\n",
    "        plt.plot(num_feats, r2_scores_train, label=\"Train\", alpha=.5)\n",
    "        plt.xlabel('Number of Features')\n",
    "        plt.ylabel('Rsquared')\n",
    "        plt.title('Rsquared by Number of Features')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "    best_cutoff = max(results, key=results.get)\n",
    "\n",
    "    #reduce X matrix\n",
    "    reduce_X = X.iloc[:, np.where((X.sum() > int(best_cutoff)) == True)[0]]\n",
    "    num_feats.append(reduce_X.shape[1])\n",
    "\n",
    "    #split the data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reduce_X, y, test_size = test_size, random_state=random_state)\n",
    "\n",
    "    #fit the model\n",
    "    lm_model = LinearRegression(normalize=True)\n",
    "    lm_model.fit(X_train, y_train)\n",
    "\n",
    "    return r2_scores_test, r2_scores_train, lm_model, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: When I included all companies and locations the find_optimal_lm_mod didn't error out, but didn't finish either.  Looks like it's taking a long time with doing the X.sum(), I think because there were so many one-hot encoded columns after dummy'ing the categorical columns, so instead I limited this investigation to companies in locations that have more than 100 respondents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cutoffs here pertains to the number of missing values allowed in the used columns.\n",
    "#Therefore, lower values for the cutoff provides more predictors in the model.\n",
    "cutoffs = [5000, 3500, 2500, 1000, 100, 50, 20, 10, 5, 4]\n",
    "\n",
    "#Run this cell to pass your X and y to the model for testing\n",
    "r2_scores_test, r2_scores_train, lm_model, X_train, X_test, y_train, y_test = find_optimal_lm_mod(X, y, cutoffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the coef_weights from the exercises to see which features have the largest weights in determining the totalyearlycompensation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coef_weights(coefficients, X_train, ascending=True):\n",
    "    '''\n",
    "    INPUT:\n",
    "    coefficients - the coefficients of the linear model \n",
    "    X_train - the training data, so the column names can be used\n",
    "    OUTPUT:\n",
    "    coefs_df - a dataframe holding the coefficient, estimate, and abs(estimate)\n",
    "    \n",
    "    Provides a dataframe that can be used to understand the most influential coefficients\n",
    "    in a linear model by providing the coefficient estimates along with the name of the \n",
    "    variable attached to the coefficient.\n",
    "    '''\n",
    "    coefs_df = pd.DataFrame()\n",
    "    coefs_df['est_int'] = X_train.columns\n",
    "    coefs_df['coefs'] = lm_model.coef_\n",
    "    coefs_df['abs_coefs'] = np.abs(lm_model.coef_)\n",
    "    coefs_df = coefs_df.sort_values('coefs', ascending=ascending)\n",
    "    return coefs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_weights(lm_model.coef_, X_train, True).head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_weights(lm_model.coef_, X_train, False).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions based on the linear model:\n",
    "\n",
    "- Location plays a central role in overall compensation, majority of locations being in California\n",
    "\n",
    "-->This agrees with the analysis done above\n",
    "\n",
    "- Company Booking.com, Nvidia and Intel have good overall compensation, whereas other companies do not like Yandex, Qualcomm, Netflix\n",
    "\n",
    "--> This result seems strange to me.  Booking.com, Nvidia, and Intel are all companies that are NOT in the top 20 for highest mean annual total compensation which was analyzed above.  Also Netflix IS in the top 20 companies with high mean annual total compensation according to the analysis above but the coefficient weight is negative meaning the model is indicating working at Netflix will have a negative impact on your overall annual compensation\n",
    "\n",
    "- Years of experience, years at the company, tag, race, education, and gender don't play as much of a central role as location and company do since their coefficients don't show up in the top 20 weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of these descrepancies I'll generally conclude from the model that:\n",
    "\n",
    "- Location and company play a key role in your total annual compensation, with companies in CA providing better compensation\n",
    "\n",
    "- I won't conclude any of the other findings based on the model since I'm unsure of it's accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
